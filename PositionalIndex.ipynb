{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PositionalIndex.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Index"
      ],
      "metadata": {
        "id": "F7PgAjD-eagt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "Lzkq_TGDegw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "QHY292UZr9AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FltWNIV0dk7h",
        "outputId": "6090cbfc-fe63-4872-9b04-88c806bdcc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj3Ec-2JuwCg",
        "outputId": "726875f7-8d7e-4039-f2c5-5eda05ac6b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "iE64P0sNemsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3kkp128r-pV",
        "outputId": "d497c789-ca38-44cb-ca25-fe9c56861dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data =json.load(open(\"/content/drive/MyDrive/IR_Assignments/docs.json\", \"r\"))\n",
        "file_types =json.load(open(\"/content/drive/MyDrive/IR_Assignments/special_docs.json\", \"r\"))"
      ],
      "metadata": {
        "id": "zMRvQS3WsAy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "gxnVGJQTethU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(input):\n",
        "    input=input.replace('\\a',' ')\n",
        "    input=input.replace('\\b',' ')\n",
        "    input=input.replace('\\f',' ')\n",
        "    input=input.replace('\\n',' ')    \n",
        "    input=input.replace('\\r',' ')\n",
        "    input=input.replace('\\t',' ')\n",
        "    input=input.replace('\\v',' ')\n",
        "    # removing special characters\n",
        "    # output = re.sub(r'[^\\x20-\\x7e]','',input)\n",
        "    # print(output)\n",
        "    # convert to lower case\n",
        "    output = input.lower()\n",
        "    # remove punctuations\n",
        "    punctuations=string.punctuation.replace(\"'\",'')\n",
        "    output = \"\".join([char if char not in punctuations else ' ' for char in output])\n",
        "    output = output.replace(\"'\",'')\n",
        "    # print(output)\n",
        "    # tokenize\n",
        "    output = nltk.word_tokenize(output)\n",
        "    # removing words with special characters\n",
        "    output = [word for word in output if re.sub(r'[^\\x20-\\x7e]','',word) == word]\n",
        "    # remove stopwords\n",
        "    output = [word.strip() for word in output if word not in nltk.corpus.stopwords.words('english')]\n",
        "    return output"
      ],
      "metadata": {
        "id": "0vcfXjqmtFaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in raw_data:\n",
        "  raw_data[doc] = preprocess(raw_data[doc])"
      ],
      "metadata": {
        "id": "jWy6D4jVo10l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating doc to doc-id mapping**"
      ],
      "metadata": {
        "id": "47C11DsAezjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_docs(raw_data):\n",
        "    doc_ids = {}\n",
        "    id = 1\n",
        "    for doc in raw_data:\n",
        "        doc_ids[doc] = id\n",
        "        id += 1\n",
        "    return doc_ids\n",
        "\n",
        "doc_ids = map_docs(raw_data)"
      ],
      "metadata": {
        "id": "z7y7r11mKfm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating positional index**"
      ],
      "metadata": {
        "id": "s3pKuu5we3bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index(doc_ids):\n",
        "  positional_index={}\n",
        "  ctr=0\n",
        "  for doc in raw_data:\n",
        "    ctr+=1\n",
        "    for i in range(len(raw_data[doc])):\n",
        "      token=raw_data[doc][i]\n",
        "      if token in positional_index.keys():\n",
        "        if doc_ids[doc] in positional_index[token].keys():\n",
        "          positional_index[token][doc_ids[doc]].append(i)\n",
        "        else:\n",
        "          positional_index[token][doc_ids[doc]]=[i]\n",
        "      else:\n",
        "        positional_index[token] = {doc_ids[doc]:[i]}\n",
        "    # if ctr%5==0:\n",
        "    #   break\n",
        "  return positional_index\n",
        "positional_index = create_index(doc_ids)"
      ],
      "metadata": {
        "id": "TTzg6URbNn86"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing phrase queries**"
      ],
      "metadata": {
        "id": "-U2U89Wne6WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process(query):\n",
        "  not_present=0\n",
        "  for word in query:\n",
        "    if word not in positional_index:\n",
        "      not_present = 1\n",
        "      break\n",
        "  count = 0\n",
        "  doc_list = []\n",
        "  if not_present == 0:\n",
        "    for doc in positional_index[query[0]]:\n",
        "      for occurance in positional_index[query[0]][doc]:\n",
        "        found = 1\n",
        "        ptr=1\n",
        "        for i in range(1,len(query)):\n",
        "          if doc not in positional_index[query[i]].keys() or (occurance+ptr) not in positional_index[query[i]][doc]:\n",
        "            found = 0\n",
        "            break\n",
        "          ptr+=1\n",
        "        if found == 1:\n",
        "          count+=1\n",
        "          doc_list.append(doc)\n",
        "          break  \n",
        "  print('Total Number of occurances =',count)\n",
        "  print('List of Documents =',doc_list)\n",
        "  key_list = list(doc_ids.keys())\n",
        "  val_list = list(doc_ids.values())\n",
        "  for doc in doc_list:\n",
        "    position = val_list.index(doc)\n",
        "    print(key_list[position])"
      ],
      "metadata": {
        "id": "apvABhDfHnG9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Input sentence: \")\n",
        "query = preprocess(query)\n",
        "process(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFmXa4LB13QJ",
        "outputId": "08e86ddf-4198-47c3-ba07-6c31848f63a0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: good morning\n",
            "Total Number of occurances = 14\n",
            "List of Documents = [72, 231, 425, 481, 538, 638, 786, 840, 1002, 1004, 1005, 1035, 1058, 1110]\n",
            "bad.jok\n",
            "coffeebeerwomen.txt\n",
            "gd_ql.txt\n",
            "homermmm.txt\n",
            "jason.fun\n",
            "math.2\n",
            "phorse.hum\n",
            "pun.txt\n",
            "teevee.hum\n",
            "televisi.hum\n",
            "televisi.txt\n",
            "top10st1.txt\n",
            "t_zone.jok\n",
            "worldend.hum\n"
          ]
        }
      ]
    }
  ]
}